
save costs: take down support page and apache web server
remove stale nodes (**** see PEER REPORTING OF OTHER PEERS THAT APPEAR OFFLINE below)
stay-in-touch with peers (and self nodes?)

fire chat_hole_punch from case: CHAT_STATUS_ATTEMPTING_HOLE_PUNCH...

iOS: show if a user is online

lowercase letters only for username including
	search hashtbl keys

change keys for RSA and AES on ios app, to see if other bugs come up for generating new keys

!!!! PEER REPORTING OF OTHER PEERS THAT APPEAR OFFLINE
	- this should fix the biggest bug
	- implement stay-in-touch between peers (nodes)
		- when peer A doesn't recv n number of stay-in-touch msgs from peer B, then peer A reports peer B to the server as offline
		- on the server, add a status for reporting other peers offline
		- on the server, you should implement logic for actually removing a node...
			e.g. if all of its contacts' nodes report the same thing
			or this might be better: if x% of its contacts' nodes report the same thing

pretty ios POC app
database
install on server
make sure server is ALWAYS running

*******************  POC is done  **********************

FIX-ALL-BUGS
----------------------------------------------
************* UDP guarantee (i.e. packet loss check and resend) *************

	second login (i.e. without cntrl-c'ing the-server) is broken
		-after add/accept contact request
		-chatting with existing contact
		-backgrounded?
		-all time time? 

infrequently and seemingly randomly, when performing internal hole punching, it seems a node has an internal ip4 = 0.0.0.0... i see this manifest as "send_chat_hole_punch to 0.0.0.0"... and so the hole punching fails.... to catch this, log the following:
	- STATUS_INIT_NODE on the server, capture the ip4 from node_buf_t... this is the internal ip of the client... is this 0.0.0.0?
	- notify_existing_peer_of_new_chat_port on the server... log the ip4 we're sending to each client... is this 0.0.0.0?
	- on the client, STATUS_PROCEED_CHAT_HP, log the ip4 we receive from the server... is this 0.0.0.0?

backgrpund the app and quickly (less than 3 seconds) foreground the app... on the server, notice the logs: it doesn't recognize the authn token from stay-in-touch ping... this is because backgrounding the app tosses out the authn token and re-authn's... and the reason that the client is still sending stay-in-touch messages with an old authn token is that I don't specifically terminate the thread WHEN the user backgrounds the app... I just set the variable stay_in_touch_running = 0 but that thread is sleeping while stay_in_touch_running = 0... So you need to correctly terminate that thread (and all threads)

slightly buggy, maybe
	create new user
	sign-out
-----------------------------------------------

email
fork/thread servers

To reduce endpoints (i.e. remove search-endpoint), can we stuff a struct of arbitrary size into the last member of type unsigned char*?

authentication
	when a user signs out
		a- the server should check if user's hashnode->nodes->node_count == 0 (i.e. user offline)
		b- if this is true, then the server should notify the user's contacts
		c- and the app should reflect updated online/offline status in the user's contacts list
		d- larger picture: each node should report changes in its status (online/asleep/busy/DoNotDisturb/Offline) to the server
		e- and the server should record that status in the hashnode->nodes->node
		f- and then notify the user's contacts' nodes of the status change

	remove node_t (from server->self-hashnode->nodes) when stay-in-touch hasn't been received after x time

	delete "stale" authn_nodes from authn hashtable
	delete "stale" token_nodes from token hashtable

	why isn't AES key being retrieved properly in ios app? something wrong in - (id)objectForKey:(id)key
	save server's RSA pub key in ios keychain? or is that not even necessary, since we should ping the authn server every session start anyway?
	encrypt AES key with RSA key
	handle new user vs existing user in ios app
	finish new user status (including password)

	handle handoff from authn server to main server
		1-add authn_token member to structs node_t and node_buf_t
		2-if new user, add hashnode and node (with authn token, ip, port, fam, etc)
		3-if existing user, add node to corresponding hashnode (with authn token, ip, port, fam)
		4-for 2 & 3, move big chunk of code from main-server->STATUS_INIT_NODE to authn-server, yeah?
	strlen + 1?
	delete items from keychain on app delete		

	do we even need an AuthN token? I guess it verifies to the main server that we're authenticated, right? If the main server doesn't find a matching AuthN token in the corresponding hashnode->node, then it rejects that node, right? Answer: copy AuthN key (concated fam, ip, port) and token to node_t and node_buf_t, and match key and token from node_t with incoming data. Ywah?

network encryption
	finish node-to-authn-server
		1- when the server responds to AUTHN_STATUS_NEW_USER
		2- when the server responds to AUTHN_STATUS_EXISTING_USER
		3- when the clients sends the user data to the server: udp_client->send_user()
	node-to-main-server (I think we can just encrypt and decrypt the entire struct node_buf_t)
	node-to-chat-server
	peer-to-peer (I think we can just encrypt and decrypt the entire struct node_buf_t)
	ACTUALLY, we can get rid of server-generated AES key: time-based reset of server AES IV... and notification to all nodes... once a day?
	ensure RSA swap request comes from Sup app (ie not from a fisher): maybe some sort of time-dependent key to pass with RSA swap

device goes to sleep or app is background... will stay-in-touch with servers continue? do we need to re-do hole punching on awake/foregrounding?

save chat history locally?

chat_buf_t, CHAT_STATUS_MSG make msg arbitrarily long by sending in multiple calls to sendto/recvfrom

go back and use memcmp to compare all unsigned char arrays (i.e. ip6, etc)
thread the server
	recvfrom -> copy details (si_other, buf, etc) and handle in threadpool
thread wain et al

can we possibly put the in-memory structure in another program? if so, then we could stagger the server program and the memory program, which would allow us to change where the memory is run from without causing a disruption in service... i.e. start another memory program, wait for it to load up (copied from current memory program?), stop the server, and repoint the server to the new memory program... this might be a bad idea... but maybe a variation could help?

change authN server to TCP to avoid packet loss, ordering problems?

server host?
shrink node_buf, auth_buf, etc by converting to base64?

stress test in-memory data
	correct HASHSIZE AND AUTHN_HASHSIZE?

security
	server firewalls
	something else?

UDP-restricted firewall (STUN and TURN)

mvp app

APNs

test IPv6 somehow
valgrind (for client, try to use udp_client_test.c on linux)
create folder client-peer
clean up buttons and extraneous functions
get rid of node_min?
